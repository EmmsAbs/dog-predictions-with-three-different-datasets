{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, i would like to thank Udacity for this opportunity. I would also like to excuse myself if i make some mistakes in my English. I studied in French and for English, iâ€™m more good at listening, reading than writing. The Project was really interesting and hard to start for me. I got a problem with my computer, then i waited for more than one week for a Twitter Api key, but i didnt get it. Then i decided to use the tweet_json file Udacity provide us. \n",
    "##### For the gathering part, gathering twiiter_archive and images datasets was very easy for me. I used the algorithms i learned in the course. But, for the tweet_json file, i didnt know how to make it as a pandas dataframe. I wanted to ask my session leader, but instead, i went on stackoverflow and finally figure it out. So the gathering part was very interesting, but it would be more instructives if i got the Twitter API keys.\n",
    "#### Now, about the Assessing part, i started it looking into the datasets manually or maybe i would say visually, to see the structure. Then i looked for info () and and describe () about each datasets, looked if they have duplicated values and null ones, print some columns value_counts to see if they have weird or invalid values. Also, for some columns, i went deeply into them to see if they got issues. Then, i wrote in a markdown cell about 8 quality issues and 2 Tidiness isuues i resolved in the cleaning part\n",
    "##### Finally, i proceeded to the best and hard part, cleaning the datasets and eventually merge them into 2 or one dataset. In my case, i merge them into one dataset. But before merging them, i cleaned them individually. I merge some columns into only one columns, deal with invalid values and completely clean the dataset i called it twitter_archive_master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
